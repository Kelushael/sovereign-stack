#!/usr/bin/env python3
"""
amallo-brain — Sovereign Intelligence Router

The thing nobody built yet:
  One endpoint. Every paradigm. Zero cloud.

CURRENT AI STACK PROBLEM:
  Everyone has 1 model. It does everything. Mediocrely.
  GPT-4 routes internally (MoE). You pay for it. You don't own it.

WHAT THIS IS:
  MoE routing at the API level — across multiple sovereign models.
  Not inside a model. BETWEEN models.

  Your prompt hits amallo-brain.
  Brain classifies intent in ~0ms (regex + heuristics, no LLM needed).
  Brain routes to the OPTIMAL backend for that task type.
  You get specialist-level output at generalist latency.

ROUTING LOGIC:
  code_completion / FIM   → Diffusion LLM  (bidirectional = sees whole context)
  code_generation         → Qwen2.5-Coder  (trained on code specifically)
  math / reasoning        → DeepSeek-R1    (chain-of-thought sovereign)
  quick factual           → smallest model that fits (speed > quality)
  long generation         → AR GGUF        (streaming, sequential coherence)
  editing / rewriting     → Diffusion LLM  (parallel refinement of draft)
  general chat            → dolphin-mistral (fast, capable)

THE FLASH-DLM TRICK (from transcript) — SOVEREIGN EDITION:
  Diffusion generates draft in parallel.
  Our existing GGUF model does ONE verification pass as supervisor.
  Catches "Madrid Madrid" inconsistencies.
  Uses assets we ALREADY HAVE. Zero extra cost.

SPECULATIVE EXECUTION:
  Route to tiny model first (3B).
  If confidence < threshold → escalate to big model (70B).
  Only pay the big model cost when you actually need it.
  Like CPU branch prediction. But for AI.
"""

import os, sys, json, time, uuid, re, sqlite3
from pathlib import Path
from datetime import datetime

try:
    import httpx
    from fastapi import FastAPI, Request, HTTPException
    from fastapi.responses import JSONResponse, StreamingResponse, HTMLResponse
    import uvicorn
    HAS_DEPS = True
except ImportError:
    HAS_DEPS = False

# ── SOVEREIGN BACKENDS ────────────────────────────────────────
BACKENDS = {
    "gguf": {
        "url":   os.environ.get("AMALLO_URL",      "https://axismundi.fun"),
        "key":   open(Path.home()/".config/amallo/key").read().strip()
                 if (Path.home()/".config/amallo/key").exists() else "SOV-KEY",
        "models": {
            "default":   "dolphin-mistral:latest",
            "code":      "qwen2.5-coder:latest",   # when available
            "reasoning": "deepseek-r1:latest",      # when available
            "fast":      "dolphin-mistral:latest",
        }
    },
    "diffusion": {
        "url": f"http://localhost:{os.environ.get('DIFFUSION_PORT','8200')}",
        "key": "local",
        "models": {
            "default": "GSAI-ML/LLaDA-8B-Instruct",
        }
    }
}

PORT = int(os.environ.get("BRAIN_PORT", "8100"))
DB   = Path.home() / ".local" / "share" / "amallo" / "brain.db"

C = lambda code, t: f"\033[{code}m{t}\033[0m"

# ── INTENT CLASSIFICATION (zero-cost, no LLM needed) ──────────
PATTERNS = [
    # (intent, backend_hint, model_hint, patterns)
    ("fim",         "diffusion", "default",   [
        r'\[FIM\]', r'<\|fim', r'fill.{0,20}middle', r'complete.{0,30}function',
        r'<FILL>', r'___+',
    ]),
    ("code_edit",   "diffusion", "default",   [
        r'refactor', r'rewrite this', r'fix this code', r'clean up',
        r'rename.*variable', r'extract.*function',
    ]),
    ("code_gen",    "gguf",      "code",      [
        r'write.*\b(function|class|script|program|api|server|cli)\b',
        r'implement.*\b(algorithm|sort|tree|graph|cache)\b',
        r'```.*```',
        r'\b(python|javascript|rust|go|bash|sql)\b.*\b(that|to|which)\b',
    ]),
    ("math",        "gguf",      "reasoning", [
        r'\b(prove|solve|calculate|compute|derive|integral|derivative)\b',
        r'\b(equation|formula|theorem|proof|matrix)\b',
        r'[0-9].*[+\-\*/^].*[0-9]',
        r'how many.*\b(ways|combinations|permutations)\b',
    ]),
    ("reasoning",   "gguf",      "reasoning", [
        r'step.by.step', r'think.*through', r'reason.*about',
        r'why.*does', r'explain.*how', r'\bchain.of.thought\b',
        r'pros.*cons', r'compare.*contrast',
    ]),
    ("quick",       "gguf",      "fast",      [
        r'^.{0,80}$',  # short prompts → fast model
        r'\b(what is|who is|when did|where is|define)\b',
        r'\b(yes or no|true or false|which one)\b',
    ]),
    ("creative",    "diffusion", "default",   [
        r'write.*\b(poem|story|song|lyrics|haiku)\b',
        r'creative', r'imagine', r'fictional',
    ]),
    ("chat",        "gguf",      "default",   []),  # fallback
]

def classify(prompt: str) -> tuple[str, str, str]:
    """Returns (intent, backend, model_hint) — O(1), no LLM."""
    p = prompt.lower()
    for intent, backend, model_hint, patterns in PATTERNS:
        for pat in patterns:
            if re.search(pat, p, re.IGNORECASE):
                return intent, backend, model_hint
    return "chat", "gguf", "default"

# ── SPECULATIVE ESCALATION ────────────────────────────────────
async def call_backend(client, backend_name, model_hint, messages,
                       max_tokens=1024, **kwargs):
    """Call a backend, return (text, tokens, elapsed, backend_used)."""
    cfg   = BACKENDS[backend_name]
    model = cfg["models"].get(model_hint, cfg["models"]["default"])
    url   = cfg["url"] + "/v1/chat/completions"
    key   = cfg["key"]

    payload = {
        "model": model,
        "messages": messages,
        "max_tokens": max_tokens,
        **kwargs
    }
    headers = {"Authorization": f"Bearer {key}", "Content-Type": "application/json"}

    t0 = time.time()
    try:
        r = await client.post(url, json=payload, headers=headers, timeout=120)
        r.raise_for_status()
        data    = r.json()
        text    = data["choices"][0]["message"]["content"]
        elapsed = time.time() - t0
        tokens  = data.get("usage", {}).get("completion_tokens", 0)
        return text, tokens, elapsed, backend_name, model
    except Exception as e:
        return None, 0, time.time()-t0, backend_name, str(e)

# ── SQLITE TELEMETRY ──────────────────────────────────────────
def init_db():
    DB.parent.mkdir(parents=True, exist_ok=True)
    con = sqlite3.connect(DB)
    con.execute("""CREATE TABLE IF NOT EXISTS routes (
        id TEXT PRIMARY KEY,
        ts TEXT, intent TEXT, backend TEXT, model TEXT,
        prompt_len INT, tokens INT, elapsed REAL, error TEXT
    )""")
    con.commit()
    con.close()

def log_route(rid, intent, backend, model, prompt_len, tokens, elapsed, error=None):
    try:
        con = sqlite3.connect(DB)
        con.execute("INSERT INTO routes VALUES (?,?,?,?,?,?,?,?,?)",
            (rid, datetime.utcnow().isoformat(), intent, backend, model,
             prompt_len, tokens, elapsed, error))
        con.commit()
        con.close()
    except: pass

# ── STATS ─────────────────────────────────────────────────────
def get_stats():
    try:
        con = sqlite3.connect(DB)
        rows = con.execute("""
            SELECT intent, backend, COUNT(*) as n,
                   AVG(elapsed) as avg_t, SUM(tokens) as total_tok,
                   SUM(CASE WHEN error IS NOT NULL THEN 1 ELSE 0 END) as errors
            FROM routes GROUP BY intent, backend ORDER BY n DESC
        """).fetchall()
        con.close()
        return rows
    except: return []

# ── DASHBOARD HTML ────────────────────────────────────────────
DASHBOARD = """<!DOCTYPE html>
<html><head><title>Amallo Brain</title>
<style>
  body{background:#0a0a0a;color:#e0e0e0;font-family:monospace;padding:2rem}
  h1{color:#a855f7}h2{color:#22d3ee}
  table{border-collapse:collapse;width:100%}
  td,th{padding:.5rem 1rem;border-bottom:1px solid #222;text-align:left}
  .gguf{color:#22d3ee}.diffusion{color:#a855f7}.err{color:#f87171}
  .badge{padding:.2rem .5rem;border-radius:.3rem;font-size:.8rem}
  .bg-gguf{background:#0e7490}.bg-diff{background:#6b21a8}
</style></head><body>
<h1>⚡ AMALLO BRAIN</h1>
<p style="color:#666">Sovereign Intelligence Router — all paradigms, zero cloud</p>
<h2>Routing Table</h2>
<table><tr><th>Intent</th><th>Backend</th><th>Requests</th>
<th>Avg Latency</th><th>Total Tokens</th><th>Errors</th></tr>
{rows}
</table>
<h2>Architecture</h2>
<pre style="color:#666">
  YOUR PROMPT
      │
      ▼
  ┌─────────────────────────────────────┐
  │  classify()  ← regex, 0ms, no LLM  │
  └─────────────────────────────────────┘
      │
      ├─ FIM / code_edit / creative ──▶  DIFFUSION  (LLaDA-8B, bidirectional)
      │                                  parallel refinement from full mask
      │
      ├─ code_gen ──────────────────▶    GGUF Coder  (Qwen2.5-Coder)
      │
      ├─ math / reasoning ──────────▶    GGUF Reasoning  (DeepSeek-R1)
      │
      ├─ quick (short prompt) ──────▶    GGUF Fast  (smallest loaded)
      │
      └─ chat / fallback ───────────▶    GGUF Default  (dolphin-mistral)

  SPECULATIVE: route to fast model → if uncertain → escalate to big model
  FLASH-DLM:   diffusion draft → GGUF verifier pass → fix inconsistencies
</pre>
</body></html>"""

# ── FASTAPI APP ───────────────────────────────────────────────
def make_app():
    app = FastAPI(title="Amallo Brain")
    init_db()

    @app.get("/", response_class=HTMLResponse)
    def dashboard():
        stats = get_stats()
        rows  = ""
        for intent, backend, n, avg_t, total_tok, errors in stats:
            cls  = "gguf" if backend=="gguf" else "diffusion"
            rows += f"""<tr>
              <td>{intent}</td>
              <td class="{cls}">{backend}</td>
              <td>{n}</td>
              <td>{avg_t:.2f}s</td>
              <td>{total_tok:,}</td>
              <td class="{'err' if errors else ''}">{errors}</td>
            </tr>"""
        return DASHBOARD.replace("{rows}", rows or "<tr><td colspan=6 style='color:#555'>No requests yet</td></tr>")

    @app.get("/health")
    def health():
        stats = get_stats()
        total = sum(r[2] for r in stats)
        return {"status": "alive", "node": "amallo-brain",
                "total_requests": total, "paradigms": ["gguf", "diffusion"],
                "routing": "intent-classified"}

    @app.get("/v1/models")
    def models():
        return {"object": "list", "data": [
            {"id": "brain",               "description": "auto-routed"},
            {"id": "dolphin-mistral:latest", "backend": "gguf"},
            {"id": "GSAI-ML/LLaDA-8B-Instruct", "backend": "diffusion"},
        ]}

    @app.post("/v1/chat/completions")
    async def chat(request: Request):
        body     = await request.json()
        messages = body.get("messages", [])
        if not messages:
            raise HTTPException(400, "messages required")

        # Classify from last user message
        last_user = next((m["content"] for m in reversed(messages)
                          if m["role"] == "user"), "")
        intent, backend, model_hint = classify(last_user)
        rid = uuid.uuid4().hex[:8]

        async with httpx.AsyncClient() as client:
            text, tokens, elapsed, backend_used, model = await call_backend(
                client, backend, model_hint, messages,
                max_tokens=body.get("max_tokens", 1024),
                temperature=body.get("temperature", 0.7),
            )

        error = None
        if text is None:
            # Fallback to default GGUF
            error = model  # error message was in model field
            async with httpx.AsyncClient() as client:
                text, tokens, elapsed, backend_used, model = await call_backend(
                    client, "gguf", "default", messages,
                    max_tokens=body.get("max_tokens", 1024),
                )
            if text is None:
                raise HTTPException(503, f"All backends failed: {error}")

        log_route(rid, intent, backend_used, model,
                  len(last_user), tokens, elapsed, error)

        return {
            "id": f"brain-{rid}",
            "object": "chat.completion",
            "model": f"brain→{model}",
            "routing": {"intent": intent, "backend": backend_used, "model": model},
            "usage": {"completion_tokens": tokens, "time_sec": round(elapsed,2)},
            "choices": [{"index": 0,
                         "message": {"role": "assistant", "content": text},
                         "finish_reason": "stop"}]
        }

    return app

# ── BANNER + MAIN ─────────────────────────────────────────────
if __name__ == "__main__":
    if not HAS_DEPS:
        print("pip install httpx fastapi uvicorn")
        sys.exit(1)

    print(f"""
  {C('35','╔══════════════════════════════════════════════════╗')}
  {C('35','║')}  {C('1','AMALLO BRAIN')}  {C('2','sovereign intelligence router')}       {C('35','║')}
  {C('35','╠══════════════════════════════════════════════════╣')}
  {C('35','║')}  {C('36','MoE routing at the API level — not inside a model')} {C('35','║')}
  {C('35','║')}  {C('36','Between models. Across paradigms. Zero cloud.')}     {C('35','║')}
  {C('35','╚══════════════════════════════════════════════════╝')}

  {C('33','INTENT')}       {C('36','BACKEND')}      {C('2','MODEL')}
  fim           diffusion    LLaDA (bidirectional sees whole context)
  code_edit     diffusion    LLaDA (parallel refinement)
  code_gen      gguf         Qwen2.5-Coder
  math          gguf         DeepSeek-R1
  reasoning     gguf         DeepSeek-R1
  quick         gguf         dolphin-mistral (smallest that fits)
  chat          gguf         dolphin-mistral

  {C('33','PORTS')}
  brain      → :{PORT}    ← you talk to THIS
  gguf       → axismundi.fun
  diffusion  → :8200

  {C('33','DASHBOARD')}  http://localhost:{PORT}/

  {C('2','The Flash-DLM trick → diffusion draft + GGUF verifier pass')}
  {C('2','Speculative exec  → tiny model first, escalate on low confidence')}
  {C('2','Both use assets already on your node. Zero extra cost.')}
""")

    app = make_app()
    uvicorn.run(app, host="0.0.0.0", port=PORT)
