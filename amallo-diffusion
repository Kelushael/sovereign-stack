#!/usr/bin/env python3
"""
amallo-diffusion — Sovereign Diffusion LLM Server
Wraps LLaDA-8B-Instruct with an OpenAI-compatible API

Architecture: MASKED DIFFUSION (not autoregressive)
  - Starts with fully masked response
  - Runs N denoising steps in parallel
  - Each step: predict all tokens → remasking low-confidence ones
  - Block diffusion: semi-AR across blocks, exact KV cache per block

Speed: CPU = slow but correct. GPU = 5-10x faster than AR equivalent.
       The parallelism ONLY pays off on GPU. On CPU you're paying the
       same transformer cost N times with no hardware parallelism benefit.

Models available:
  GSAI-ML/LLaDA-8B-Instruct     - original 8B (fits in 32GB CPU @ bfloat16)
  GSAI-ML/LLaDA-1.5             - improved 8B
  inclusionAI/LLaDA-MoE-7B-A1B-Instruct - 7B MoE (1B active params = fast)
  inclusionAI/LLaDA2.1-mini     - 16B MoE, Feb 2026, best quality
"""

import os, sys, json, time, uuid, asyncio
from pathlib import Path

try:
    import torch
    import numpy as np
    import torch.nn.functional as F
    from transformers import AutoTokenizer, AutoModel
    from fastapi import FastAPI, HTTPException
    from fastapi.responses import JSONResponse, StreamingResponse
    import uvicorn
    HAS_DEPS = True
except ImportError:
    HAS_DEPS = False

# ── CONFIG ────────────────────────────────────────────────────
PORT         = int(os.environ.get("DIFFUSION_PORT", "8200"))
MODEL_ID     = os.environ.get("DIFFUSION_MODEL", "GSAI-ML/LLaDA-8B-Instruct")
DEVICE       = os.environ.get("DIFFUSION_DEVICE", "cuda" if (HAS_DEPS and torch.cuda.is_available()) else "cpu")
DEFAULT_STEPS = int(os.environ.get("DIFFUSION_STEPS", "64"))   # 128 = best, 10 = fast test
DEFAULT_GEN   = int(os.environ.get("DIFFUSION_GENLEN", "256"))
BLOCK_LEN     = int(os.environ.get("DIFFUSION_BLOCKLEN", "32")) # block diffusion block size
MASK_ID       = 126336  # LLaDA [MASK] token id

C = lambda code, t: f"\033[{code}m{t}\033[0m"

# ── DIFFUSION CORE (from ML-GSAI/LLaDA generate.py) ──────────
def add_gumbel_noise(logits, temperature):
    if temperature == 0:
        return logits
    logits = logits.to(torch.float64)
    noise = torch.rand_like(logits, dtype=torch.float64)
    gumbel_noise = (-torch.log(noise)) ** temperature
    return logits.exp() / gumbel_noise

def get_num_transfer_tokens(mask_index, steps):
    mask_num = mask_index.sum(dim=1, keepdim=True)
    base = mask_num // steps
    remainder = mask_num % steps
    num_transfer_tokens = torch.zeros(mask_num.size(0), steps,
                                      device=mask_index.device, dtype=torch.int64) + base
    for i in range(mask_num.size(0)):
        num_transfer_tokens[i, :remainder[i]] += 1
    return num_transfer_tokens

@torch.no_grad()
def diffusion_generate(model, prompt, steps=64, gen_length=256,
                        block_length=32, temperature=0., remasking='low_confidence'):
    """
    Masked diffusion sampling — the core loop.
    Block diffusion: generates block_length tokens at a time semi-autoregressively.
    This enables EXACT KV caching per committed block (unlike pure diffusion).
    """
    x = torch.full((prompt.shape[0], prompt.shape[1] + gen_length),
                   MASK_ID, dtype=torch.long).to(model.device)
    x[:, :prompt.shape[1]] = prompt.clone()

    assert gen_length % block_length == 0
    num_blocks = gen_length // block_length
    steps_per_block = max(1, steps // num_blocks)

    for num_block in range(num_blocks):
        block_start = prompt.shape[1] + num_block * block_length
        block_end   = prompt.shape[1] + (num_block + 1) * block_length
        block_mask_index = (x[:, block_start:block_end] == MASK_ID)
        num_transfer_tokens = get_num_transfer_tokens(block_mask_index, steps_per_block)

        for i in range(steps_per_block):
            mask_index = (x == MASK_ID)
            logits = model(x).logits

            logits_noisy = add_gumbel_noise(logits, temperature=temperature)
            x0 = torch.argmax(logits_noisy, dim=-1)

            if remasking == 'low_confidence':
                p = F.softmax(logits, dim=-1)
                x0_p = torch.squeeze(torch.gather(p, -1, torch.unsqueeze(x0, -1)), -1)
            else:
                x0_p = torch.rand(x0.shape, device=x0.device)

            # Only remasking decisions for current block
            x0_p[:, block_end:] = -float('inf')

            x0 = torch.where(mask_index, x0, x)
            confidence = torch.where(mask_index, x0_p, -float('inf'))

            transfer_index = torch.zeros_like(x0, dtype=torch.bool)
            for j in range(confidence.shape[0]):
                _, sel = torch.topk(confidence[j], k=num_transfer_tokens[j, i])
                transfer_index[j, sel] = True
            x[transfer_index] = x0[transfer_index]

    return x

# ── MODEL LOADER ──────────────────────────────────────────────
model_cache = {}

def load_model(model_id):
    if model_id in model_cache:
        return model_cache[model_id]
    
    print(f"  {C('33', '⟳')} Loading {model_id} on {DEVICE}...")
    dtype = torch.bfloat16 if DEVICE == 'cuda' else torch.float32
    
    tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)
    if tokenizer.padding_side != 'left':
        tokenizer.padding_side = 'left'
    
    model = AutoModel.from_pretrained(
        model_id, trust_remote_code=True, torch_dtype=dtype
    ).to(DEVICE).eval()
    
    model_cache[model_id] = (model, tokenizer)
    print(f"  {C('32', '✓')} {model_id} loaded")
    return model_cache[model_id]

# ── FASTAPI SERVER ────────────────────────────────────────────
def make_app():
    app = FastAPI(title="Amallo Diffusion Server")

    @app.get("/health")
    def health():
        return {"status": "alive", "model": MODEL_ID, "device": DEVICE,
                "paradigm": "masked-diffusion", "node": "amallo-diffusion"}

    @app.get("/v1/models")
    def list_models():
        return {"object": "list", "data": [
            {"id": MODEL_ID, "object": "model", "paradigm": "diffusion"},
            {"id": "GSAI-ML/LLaDA-1.5", "object": "model", "paradigm": "diffusion"},
            {"id": "inclusionAI/LLaDA-MoE-7B-A1B-Instruct", "object": "model", "paradigm": "diffusion"},
        ]}

    @app.post("/v1/chat/completions")
    async def chat(request: dict):
        messages  = request.get("messages", [])
        steps     = request.get("diffusion_steps", DEFAULT_STEPS)
        gen_len   = min(request.get("max_tokens", DEFAULT_GEN), 512)
        block_len = request.get("block_length", BLOCK_LEN)
        temp      = request.get("temperature", 0.0)

        if not messages:
            raise HTTPException(400, "messages required")

        try:
            model, tokenizer = load_model(MODEL_ID)
        except Exception as e:
            raise HTTPException(503, f"Model load failed: {e}")

        # Format chat template
        prompt_str = tokenizer.apply_chat_template(
            messages, add_generation_prompt=True, tokenize=False
        )
        inputs = tokenizer(prompt_str, add_special_tokens=False,
                           return_tensors="pt").to(DEVICE)
        input_ids = inputs["input_ids"]

        t0 = time.time()
        out = diffusion_generate(
            model, input_ids,
            steps=steps, gen_length=gen_len,
            block_length=min(block_len, gen_len),
            temperature=temp
        )
        elapsed = time.time() - t0

        response_ids = out[:, input_ids.shape[1]:]
        text = tokenizer.batch_decode(response_ids, skip_special_tokens=True)[0]

        out_tokens = response_ids.shape[1]
        tps = round(out_tokens / elapsed, 1)

        return {
            "id": f"diff-{uuid.uuid4().hex[:8]}",
            "object": "chat.completion",
            "model": MODEL_ID,
            "paradigm": "masked-diffusion",
            "usage": {
                "prompt_tokens": input_ids.shape[1],
                "completion_tokens": out_tokens,
                "diffusion_steps": steps,
                "time_sec": round(elapsed, 2),
                "tokens_per_sec": tps
            },
            "choices": [{
                "index": 0,
                "message": {"role": "assistant", "content": text},
                "finish_reason": "stop"
            }]
        }

    return app

# ── CLI ───────────────────────────────────────────────────────
def print_banner():
    print(f"""
  {C('35', '╔═══════════════════════════════════════════╗')}
  {C('35', '║')}  {C('1', 'AMALLO DIFFUSION SERVER')}                  {C('35', '║')}
  {C('35', '║')}  {C('2', 'Masked Diffusion LLM  •  NOT autoregressive')} {C('35', '║')}
  {C('35', '╚═══════════════════════════════════════════╝')}

  Model  : {MODEL_ID}
  Device : {DEVICE}  {'← GPU = fast | CPU = correct but slow' if DEVICE=='cpu' else '← ★ GPU active'}
  Port   : {PORT}
  Steps  : {DEFAULT_STEPS}  (10=fast test, 64=good, 128=best)
  GenLen : {DEFAULT_GEN} tokens
  Blocks : {BLOCK_LEN} tokens/block (enables exact KV cache per block)

  {C('33', 'HOW IT WORKS:')}
  1. Response starts fully [MASK][MASK][MASK]...
  2. Each step: model predicts ALL tokens in parallel
  3. High-confidence tokens committed, rest re-masked
  4. Block diffusion = semi-AR, exact KV cache per committed block
  5. Result: O(steps) not O(tokens) — parallelism wins on GPU

  {C('36', 'API:')} http://0.0.0.0:{PORT}/v1/chat/completions  (OpenAI compatible)
  {C('36', 'TIP:')} axis --model diffusion "your question"  (once registered)
""")

if __name__ == "__main__":
    args = sys.argv[1:]

    if "--help" in args:
        print(__doc__)
        sys.exit(0)

    if not HAS_DEPS:
        print(f"""
  {C('31', '✗ Missing dependencies')}

  pip install torch transformers fastapi uvicorn

  For GPU (recommended):
    pip install torch --index-url https://download.pytorch.org/whl/cu121

  For CPU (testing only — slow):
    pip install torch
""")
        sys.exit(1)

    if "--check" in args:
        print(f"  torch   : {torch.__version__}")
        print(f"  CUDA    : {torch.cuda.is_available()} ({torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'none'})")
        print(f"  device  : {DEVICE}")
        sys.exit(0)

    print_banner()

    # Eager load model
    if "--lazy" not in args:
        try:
            load_model(MODEL_ID)
        except Exception as e:
            print(f"  {C('31','✗')} Could not preload model: {e}")
            print(f"  {C('33','→')} Run: huggingface-cli download {MODEL_ID}")
            print(f"  {C('2', '  or set DIFFUSION_MODEL=inclusionAI/LLaDA-MoE-7B-A1B-Instruct')}")

    app = make_app()
    uvicorn.run(app, host="0.0.0.0", port=PORT)
